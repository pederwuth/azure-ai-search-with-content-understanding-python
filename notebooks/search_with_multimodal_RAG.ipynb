{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Retrieval Augmented Generation with Content Understanding\n",
    "\n",
    "# Overview\n",
    "\n",
    "Azure AI Content Understanding provides a powerful solution for extracting data from diverse content types, while preserving semantic integrity and contextual relationships ensuring optimal performance in Retrieval Augmented Generation (RAG) applications.\n",
    "\n",
    "This sample demonstrates how to leverage Azure AI's Content Understanding capabilities to extract:\n",
    "\n",
    "- OCR and layout information from documents\n",
    "- Image description, summarization and classification\n",
    "- Audio transcription with speaker diarization from audio files\n",
    "- Shot detection, keyframe extraction, and audio transcription from videos\n",
    "\n",
    "This notebook illustrates how to extract content from unstructured multimodal data and apply it to Retrieval Augmented Generation (RAG). The resulting output can be converted to vector embeddings and indexed in Azure AI Search. When a user submits a query, Azure AI Search retrieves relevant chunks to generate a context-aware response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "1. Follow [README](../README.md#configure-azure-ai-service-resource) to create essential resource that will be used in this sample\n",
    "2. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load and validate Azure AI Services configs\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\") or \"2024-12-01-preview\"\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_VERSION = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_VERSION\") or \"2024-11-30\"\n",
    "\n",
    "# Load and validate Azure OpenAI configs\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\") or \"2024-08-01-preview\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\") or \"2023-05-15\"\n",
    "\n",
    "# Load and validate Azure Search Services configs\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") or \"sample-doc-index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create analyzers with pre-defined schemas.\n",
    "Feel free to start with the provided sample data as a reference and experiment with your own data to explore its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "#set analyzer configs\n",
    "analyzer_configs = [\n",
    "    {\n",
    "        \"id\": \"doc-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/content_document.json\",\n",
    "        \"location\": Path(\"../data/sample_layout.pdf\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"image-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/image_chart_diagram_understanding.json\",\n",
    "        \"location\": Path(\"../data/sample_report.pdf\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"audio-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/call_recording_analytics.json\",\n",
    "        \"location\": Path(\"../data/callCenterRecording.mp3\"),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"video-analyzer\" + str(uuid.uuid4()),\n",
    "        \"template_path\": \"../analyzer_templates/video_content_understanding.json\",\n",
    "        \"location\": Path(\"../data/FlightSimulator.mp4\"),\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create Content Understanding client\n",
    "content_understanding_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/content_extraction\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "# Iterate through each config and create an analyzer\n",
    "for analyzer in analyzer_configs:\n",
    "    analyzer_id = analyzer[\"id\"]\n",
    "    template_path = analyzer[\"template_path\"]\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Create the analyzer using the content understanding client\n",
    "        response = content_understanding_client.begin_create_analyzer(\n",
    "            analyzer_id=analyzer_id,\n",
    "            analyzer_template_path=template_path\n",
    "        )\n",
    "        result = content_understanding_client.poll_result(response)\n",
    "        print(f\"Successfully created analyzer: {analyzer_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create analyzer: {analyzer_id}\")\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use created analyzers to extract multimodal content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Iterate through each analyzer created and analyze content for each modality\n",
    "\n",
    "analyzer_results =[]\n",
    "extracted_markdown = []\n",
    "analyzer_content = []\n",
    "for analyzer in analyzer_configs:\n",
    "    analyzer_id = analyzer[\"id\"]\n",
    "    template_path = analyzer[\"template_path\"]\n",
    "    file_location = analyzer[\"location\"]\n",
    "    try:\n",
    "           # Analyze content\n",
    "            response = content_understanding_client.begin_analyze(analyzer_id, file_location)\n",
    "            result = content_understanding_client.poll_result(response)\n",
    "            analyzer_results.append({\"id\":analyzer_id, \"result\": result[\"result\"]})\n",
    "            analyzer_content.append({\"id\": analyzer_id, \"content\": result[\"result\"][\"contents\"]})\n",
    "                       \n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error in creating analyzer. Please double-check your analysis settings.\\nIf there is a conflict, you can delete the analyzer and then recreate it, or move to the next cell and use the existing analyzer.\")\n",
    "\n",
    "print(\"Analyzer Results:\")\n",
    "for analyzer_result in analyzer_results:\n",
    "    print(f\"Analyzer ID: {analyzer_result['id']}\")\n",
    "    print(json.dumps(analyzer_result[\"result\"], indent=2))            \n",
    "\n",
    "# Delete the analyzer if it is no longer needed\n",
    "#content_understanding_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize multimodal data\n",
    "This is a simple starting point. Feel free to give your own chunking strategies a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess JSON output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values_to_strings(json_obj):\n",
    "    return [str(value) for value in json_obj]\n",
    "\n",
    "#process all content and convert to string      \n",
    "def process_allJSON_content(all_content):\n",
    "\n",
    "    # Initialize empty list to store string of all content\n",
    "    output = []\n",
    "\n",
    "    document_splits = [\n",
    "        \"This is a json string representing a document with text and metadata for the file located in \"+str(analyzer_configs[0][\"location\"])+\" \"\n",
    "        + v \n",
    "        + \"```\"\n",
    "        for v in convert_values_to_strings(all_content[0][\"content\"])\n",
    "    ]\n",
    "    docs = [Document(page_content=v) for v in document_splits]\n",
    "    output += docs\n",
    "\n",
    "    #convert image json object to string and append file metadata to the string\n",
    "    image_splits = [\n",
    "       \"This is a json string representing an image verbalization and OCR extraction for the file located in \"+str(analyzer_configs[1][\"location\"])+\" \"\n",
    "       + v\n",
    "       + \"```\"\n",
    "       for v in convert_values_to_strings(all_content[1][\"content\"])\n",
    "    ]\n",
    "    image = [Document(page_content=v) for v in image_splits]\n",
    "    output+=image\n",
    "\n",
    "    #convert audio json object to string and append file metadata to the string\n",
    "    audio_splits = [\n",
    "        \"This is a json string representing an audio segment with transcription for the file located in \"+str(analyzer_configs[2][\"location\"])+\" \" \n",
    "       + v\n",
    "       + \"```\"\n",
    "       for v in convert_values_to_strings(all_content[2][\"content\"])\n",
    "    ]\n",
    "    audio = [Document(page_content=v) for v in audio_splits]\n",
    "    output += audio\n",
    "\n",
    "    #convert video json object to string and append file metadata to the string\n",
    "    video_splits = [\n",
    "        \"The following is a json string representing a video segment with scene description and transcript for the file located in \"+str(analyzer_configs[3][\"location\"])+\" \"\n",
    "        + v\n",
    "        + \"```\"\n",
    "        for v in convert_values_to_strings(all_content[3][\"content\"])\n",
    "    ]\n",
    "    video = [Document(page_content=v) for v in video_splits]\n",
    "    output+=video    \n",
    "    \n",
    "    return output\n",
    "\n",
    "all_splits = process_allJSON_content(analyzer_content)\n",
    "\n",
    "print(\"There are \" + str(len(all_splits)) + \" documents.\") \n",
    "# Print the content of all doc splits\n",
    "for doc in all_splits:\n",
    "    print(f\"doc content\", doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Optional* - Split document markdown into semantic chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure langchain text splitting settings\n",
    "EMBEDDING_CHUNK_SIZE = 512\n",
    "EMBEDDING_CHUNK_OVERLAP = 20\n",
    "\n",
    "# Split the document into chunks base on markdown headers.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = analyzer_content[0]['content'][0]['markdown'] #extract document analyzer markdown (first item in the list) is the document analyzer markdown output\n",
    "docs_splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(docs_splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed and index the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the splitted documents and insert into Azure Search vector store\n",
    "def embed_and_index_chunks(docs):\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "        openai_api_version=AZURE_OPENAI_EMBEDDING_API_VERSION,  # e.g., \"2023-12-01-preview\"\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        azure_ad_token_provider=token_provider\n",
    "    )\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "        azure_search_key=None,\n",
    "        index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "        embedding_function=aoai_embeddings.embed_query\n",
    "    )\n",
    "    vector_store.add_documents(documents=docs)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# embed and index the docs:\n",
    "vector_store = embed_and_index_chunks(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve relevant chunks based on a question\n",
    "#### Execute a pure vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your query\n",
    "query = \"japan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity search\n",
    "docs = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute hybrid search. Vector and nonvector text fields are queried in parallel, results are merged, and top matches of the unified result set are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a hybrid search using the search_type parameter\n",
    "docs = vector_store.hybrid_search(query=query, k=3)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "We can utilize OpenAI GPT completion models + Azure Search to conversationally search for and chat about the results. (If you are using GitHub Codespaces, there will be an input prompt near the top of the screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rag chain\n",
    "prompt_str = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "def setup_rag_chain(vector_store):\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", k=3)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "    llm = AzureChatOpenAI(\n",
    "        openai_api_version=AZURE_OPENAI_CHAT_API_VERSION,\n",
    "        azure_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "        azure_ad_token_provider=token_provider,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# Setup conversational search\n",
    "def conversational_search(rag_chain, query):\n",
    "    print(rag_chain.invoke(query))\n",
    "\n",
    "\n",
    "rag_chain = setup_rag_chain(vector_store)\n",
    "while True:\n",
    "    query = input(\"Enter your query: \")\n",
    "    if query==\"\":\n",
    "        break\n",
    "    conversational_search(rag_chain, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
