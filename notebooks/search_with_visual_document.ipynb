{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Document Search with Azure Content Understanding\n",
    "## Objective\n",
    "This document illustrates an example workflow for how to leverage the Azure AI Content Understanding API to enhance the quality of document search.\n",
    "\n",
    "The sample will demonstrate the following steps:\n",
    "1. Extract the layout and content of a document using Azure AI Document Intelligence.\n",
    "2. For each figure in the document, extract its content with a custom analyzer using Azure AI Content Understanding, and insert it into the corresponding location in the document content.\n",
    "2. Chunk and embed the document content with LangChain and Azure OpenAI, and index them with Azure Search to generate an Azure Search index.\n",
    "3. Utilize an OpenAI chat model to search through content in the document with a natural language query.\n",
    "\n",
    "\n",
    "## Pre-requisites\n",
    "1. Follow the [README](../README.md#configure-azure-ai-service-resource) to create the required resources for this sample.\n",
    "1. Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Load and validate Azure AI Services configs\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\") or \"2024-12-01-preview\"\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_VERSION = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_VERSION\") or \"2024-11-30\"\n",
    "\n",
    "# Load and validate Azure OpenAI configs\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\") or \"2024-08-01-preview\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\") or \"2023-05-15\"\n",
    "\n",
    "# Load and validate Azure Search Services configs\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") or \"sample-index-visual-doc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the path to the file that will be analyzed\n",
    "# Sample report source: https://www.imf.org/en/Publications/CR/Issues/2024/07/18/United-States-2024-Article-IV-Consultation-Press-Release-Staff-Report-and-Statement-by-the-552100\n",
    "file = Path(\"../data/sample_report.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom analyzer using chart and diagram understanding template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# Get path to sample template\n",
    "ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/image_chart_diagram_understanding.json\"\n",
    "\n",
    "# Create analyzer\n",
    "ANALYZER_ID = \"content-understanding-search-sample-\" + str(uuid.uuid4())\n",
    "content_understanding_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/search_with_visusal_document\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = content_understanding_client.begin_create_analyzer(ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    print(f'Analyzer details for {result[\"result\"][\"analyzerId\"]}:')\n",
    "    print(json.dumps(result, indent=2))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Error in creating analyzer. Please double-check your analysis settings.\\nIf there is a conflict, you can delete the analyzer and then recreate it, or move to the next cell and use the existing analyzer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze document layout and compose with figure descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "import fitz\n",
    "from PIL import Image\n",
    "\n",
    "# Define helper functions for document-figure composition\n",
    "def insert_figure_contents(md_content, figure_contents, span_offsets):\n",
    "    \"\"\"\n",
    "    Inserts the figure content for each of the provided figures in figure_contents\n",
    "    before the span offset of that figure in the given markdown content.\n",
    "\n",
    "    Args:\n",
    "    - md_content (str): The original markdown content.\n",
    "    - figure_contents (list[str]): The contents of each figure to insert.\n",
    "    - span_offsets (list[int]): The span offsets of each figure in order. These should be sorted and strictly increasing.\n",
    "\n",
    "    Returns:\n",
    "    - str: The modified markdown content with the the figure contents prepended to each figure's span.\n",
    "    \"\"\"\n",
    "    # NOTE: In this notebook, we only alter the Markdown content returned by the Document Intelligence API,\n",
    "    # and not the per-element spans in the API response. Thus, after figure content insertion, these per-element spans will be inaccurate.\n",
    "    # This may impact use cases like citation page number calculation.\n",
    "    # Additional code may be needed to correct the spans or otherwise infer the page numbers for each citation.\n",
    "    # The main purpose of the notebook is to show the feasibility of using Content Understanding with Azure Search for RAG chat applications.\n",
    "\n",
    "    # Validate span_offsets are sorted and strictly increasing\n",
    "    if span_offsets != sorted(span_offsets) or not all([o < span_offsets[i + 1] for i, o in enumerate(span_offsets) if i < len(span_offsets) - 1]):\n",
    "        raise ValueError(\"span_offsets should be sorted and strictly increasing.\")\n",
    "\n",
    "    # Split the content based on the provided spans\n",
    "    parts = []\n",
    "    preamble = None\n",
    "    for i, offset in enumerate(span_offsets):\n",
    "        if i == 0 and offset > 0:\n",
    "            preamble = md_content[0:offset]\n",
    "            parts.append(md_content[offset:span_offsets[i + 1]])\n",
    "        elif i == len(span_offsets) - 1:\n",
    "            parts.append(md_content[offset:])\n",
    "        else:\n",
    "            parts.append(md_content[offset:span_offsets[i + 1]])\n",
    "\n",
    "    # Join the parts back together with the figure content inserted\n",
    "    modified_content = \"\"\n",
    "    if preamble:\n",
    "        modified_content += preamble\n",
    "    for i, part in enumerate(parts):\n",
    "        modified_content += f\"<!-- FigureContent=\\\"{figure_contents[i]}\\\" -->\" + part\n",
    "\n",
    "    return modified_content\n",
    "\n",
    "def crop_image_from_pdf_page(pdf_path, page_number, bounding_box):\n",
    "    \"\"\"\n",
    "    Crops a region from a given page in a PDF and returns it as an image.\n",
    "\n",
    "    Args:    \n",
    "    - pdf_path (pathlib.Path): Path to the PDF file.\n",
    "    - page_number (int): The page number to crop from (0-indexed).\n",
    "    - bounding_box (tuple): A tuple of (x0, y0, x1, y1) coordinates for the bounding box.\n",
    "    \n",
    "    Returns:\n",
    "    - PIL.Image: A PIL Image of the cropped area.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(page_number)\n",
    "    \n",
    "    # Cropping the page. The rect requires the coordinates in the format (x0, y0, x1, y1).\n",
    "    bbx = [x * 72 for x in bounding_box]\n",
    "    rect = fitz.Rect(bbx)\n",
    "    pix = page.get_pixmap(matrix=fitz.Matrix(300 / 72, 300 / 72), clip=rect)\n",
    "    \n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    \n",
    "    doc.close()\n",
    "\n",
    "    return img\n",
    "\n",
    "def format_content_understanding_result(content_understanding_result):\n",
    "    \"\"\"\n",
    "    Formats the JSON output of the Content Understanding result as Markdown for downstream usage in text.\n",
    "    \n",
    "    Args:\n",
    "    - content_understanding_result (dict): A dictionary containing the output from Content Understanding.\n",
    "\n",
    "    Returns:\n",
    "    - str: A Markdown string of the result content.\n",
    "    \"\"\"\n",
    "    def _format_result(key, result):\n",
    "        result_type = result[\"type\"]\n",
    "        if result_type in [\"string\", \"integer\", \"number\", \"boolean\"]:\n",
    "            return f\"**{key}**: \" + str(result[f'value{result_type.capitalize()}']) + \"\\n\"\n",
    "        elif result_type == \"array\":\n",
    "            return f\"**{key}**: \" + ', '.join([str(result[\"valueArray\"][i][f\"value{r['type'].capitalize()}\"]) for i, r in enumerate(result[\"valueArray\"])]) + \"\\n\"\n",
    "        elif result_type == \"object\":\n",
    "            return f\"**{key}**\\n\" + ''.join([_format_result(f\"{key}.{k}\", result[\"valueObject\"][k]) for k in result[\"valueObject\"]])\n",
    "\n",
    "    fields = content_understanding_result['result']['contents'][0]['fields']\n",
    "    markdown_result = \"\"\n",
    "    for field in fields:\n",
    "        markdown_result += _format_result(field, fields[field])\n",
    "\n",
    "    return markdown_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Run Content Understanding on each figure, format figure contents, and insert figure contents into corresponding document locations\n",
    "with open(file, 'rb') as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "        api_version=AZURE_DOCUMENT_INTELLIGENCE_API_VERSION,\n",
    "        credential=credential,\n",
    "        output=str('figures')\n",
    "    )\n",
    "\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\",\n",
    "        AnalyzeDocumentRequest(bytes_source=pdf_bytes),\n",
    "        output=[str('figures')],\n",
    "        features=['ocrHighResolution'],\n",
    "        output_content_format=\"markdown\"\n",
    "    )\n",
    "\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    \n",
    "    md_content = result.content\n",
    "\n",
    "    figure_contents = []\n",
    "    if result.figures:\n",
    "        print(\"Extracting figure contents with Content Understanding.\")\n",
    "        for figure_idx, figure in enumerate(result.figures):\n",
    "            for region in figure.bounding_regions:\n",
    "                    # Uncomment the below to print out the bounding regions of each figure\n",
    "                    # print(f\"Figure {figure_idx + 1} body bounding regions: {region}\")\n",
    "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                    bounding_box = (\n",
    "                            region.polygon[0],  # x0 (left)\n",
    "                            region.polygon[1],  # y0 (top\n",
    "                            region.polygon[4],  # x1 (right)\n",
    "                            region.polygon[5]   # y1 (bottom)\n",
    "                        )\n",
    "            page_number = figure.bounding_regions[0]['pageNumber']\n",
    "            cropped_img = crop_image_from_pdf_page(file, page_number - 1, bounding_box)\n",
    "\n",
    "            os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "            figure_filename = f\"figure_{figure_idx + 1}.png\"\n",
    "            # Full path for the file\n",
    "            figure_filepath = os.path.join(\"figures\", figure_filename)\n",
    "\n",
    "            # Save the figure\n",
    "            cropped_img.save(figure_filepath)\n",
    "            bytes_io = io.BytesIO()\n",
    "            cropped_img.save(bytes_io, format='PNG')\n",
    "            cropped_img = bytes_io.getvalue()\n",
    "\n",
    "            # Collect formatted content from the figure\n",
    "            content_understanding_response = content_understanding_client.begin_analyze(ANALYZER_ID, figure_filepath)\n",
    "            content_understanding_result = content_understanding_client.poll_result(content_understanding_response, timeout_seconds=1000)\n",
    "            figure_content = format_content_understanding_result(content_understanding_result)\n",
    "            figure_contents.append(figure_content)\n",
    "            print(f\"Figure {figure_idx + 1} contents:\\n{figure_content}\")\n",
    "\n",
    "        # Insert figure content into corresponding location in document\n",
    "        md_content = insert_figure_contents(md_content, figure_contents, [f.spans[0][\"offset\"] for f in result.figures])\n",
    "    \n",
    "    # Save results as a JSON file to cache the result for downstream use\n",
    "    result.content = md_content\n",
    "    output = {}\n",
    "    output['analyzeResult'] = result.as_dict()\n",
    "    output = json.dumps(output)\n",
    "    with open('sample_report.cache', 'w') as f:\n",
    "        f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the first line below to load in a previously cached result.\n",
    "# output = open(\"sample_report.cache\").read()\n",
    "document_content = json.loads(output)\n",
    "document_content = document_content['analyzeResult']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk text by splitting with Markdown header splitting and recursive character splitting\n",
    "This is a simple starting point. Feel free to give your own chunking strategies a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure langchain text splitting settings\n",
    "EMBEDDING_CHUNK_SIZE = 512\n",
    "EMBEDDING_CHUNK_OVERLAP = 20\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\")\n",
    "]\n",
    "\n",
    "# First split text using Markdown headers\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "chunks = text_splitter.split_text(document_content)\n",
    "\n",
    "# Then further split the text using recursive character text splitting\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(separators=[\"<!--\", \"\\n\\n\", \"#\"], chunk_size=EMBEDDING_CHUNK_SIZE, chunk_overlap=EMBEDDING_CHUNK_OVERLAP, is_separator_regex=True)\n",
    "chunks = char_text_splitter.split_documents(chunks)\n",
    "\n",
    "print(\"Number of chunks: \" + str(len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query vector index to retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "aoai_embeddings = AzureOpenAIEmbeddings(model=AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME,\n",
    "                                        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "                                        azure_ad_token_provider=token_provider,\n",
    "                                        api_version=AZURE_OPENAI_EMBEDDING_API_VERSION)\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    azure_search_key=None,\n",
    "    index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "    embedding_function=aoai_embeddings.embed_query\n",
    ")\n",
    "\n",
    "# This is a one-time operation to add the documents to the vector store. Comment out this line if you are re-running this cell with the same index.\n",
    "vector_store.add_documents(documents=chunks)\n",
    "\n",
    "# Set up the retriever that will be used to query the index for similar documents\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant documents\n",
    "query = \"What was the crude oil production in 2019?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# Print retrieved documents\n",
    "for doc in retrieved_docs:\n",
    "    print(\"Document id:\", doc.metadata['id'])\n",
    "    print(\"Content:\", doc.page_content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answer to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt template for chat model\n",
    "prompt = \"\"\"\n",
    "You are an expert in document analysis. You are proficient in reading and analyzing technical reports. You are good at numerical reasoning and have a good understanding of financial concepts. You are given a question which you need to answer based on the references provided. To answer this question, you may first read the question carefully to know what information is required or helpful to answer the question. Then, you may read the references to find the relevant information.\n",
    "\n",
    "If you find enough information to answer the question, you can first write down your thinking process and then provide a concise answer at the end.\n",
    "If you find that there is not enough information to answer the question, you can state that there is insufficient information.\n",
    "If you are not able or sure how to answer the question, say that you are not able to answer the question.\n",
    "Do not provide any information that is not present in the references.\n",
    "References are in markdown format, you may follow the markdown syntax to better understand the references.\n",
    "\n",
    "---\n",
    "References:\n",
    "{context}\n",
    "---\n",
    "\n",
    "Now, here is the question:\n",
    "---\n",
    "Question:\n",
    "{question}\n",
    "---\n",
    "Thinking Process::: \n",
    "Answer::: \n",
    "\"\"\"\n",
    "\n",
    "# Helper function to generate the formatted context from each retrieved document\n",
    "def generate_context(chunks):\n",
    "    context = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        s = (f'Source {i} Metadata: {chunk.metadata}\\n'\n",
    "                f'Source {i} Content: {chunk.page_content}')\n",
    "        context.append(s)\n",
    "    context = '\\n---\\n'.join(context)\n",
    "    return context\n",
    "\n",
    "# Remove redundant chunks\n",
    "appeared = set()\n",
    "unique_chunks = []\n",
    "for chunk in retrieved_docs:\n",
    "    chunk_id = chunk.metadata['id']\n",
    "    if chunk_id not in appeared:\n",
    "        appeared.add(chunk_id)\n",
    "        unique_chunks.append(chunk)\n",
    "context = generate_context(unique_chunks)\n",
    "\n",
    "# Format the prompt with the provided query and formatted context\n",
    "prompt = prompt.format(question=query,\n",
    "                       context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "chat_llm = AzureChatOpenAI(model=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "                            azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "                            azure_ad_token_provider=token_provider,\n",
    "                            api_version=AZURE_OPENAI_CHAT_API_VERSION,\n",
    "                            temperature=0.7)\n",
    "\n",
    "# Print the LLM's answer to the query with the retrieved documents as additional context\n",
    "answer = chat_llm.invoke(prompt)\n",
    "print(answer.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
