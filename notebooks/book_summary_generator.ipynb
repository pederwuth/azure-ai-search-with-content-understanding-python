{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3413d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Python environment...\n",
      "Python version: 3.11.13 (main, Aug 12 2025, 23:09:33) [GCC 14.2.0]\n",
      "‚úÖ Python is working!\n"
     ]
    }
   ],
   "source": [
    "# Quick test to see if Python is working\n",
    "print(\"üß™ Testing Python environment...\")\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"‚úÖ Python is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cec1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing imports...\n",
      "‚úÖ Basic Python modules imported\n",
      "‚úÖ dotenv imported\n",
      "‚úÖ Azure identity imported\n",
      "‚úÖ LangChain imported\n",
      "‚úÖ tiktoken imported\n",
      "üéØ Import test complete!\n",
      "‚úÖ LangChain imported\n",
      "‚úÖ tiktoken imported\n",
      "üéØ Import test complete!\n"
     ]
    }
   ],
   "source": [
    "# Test imports one by one to find the issue\n",
    "print(\"Testing imports...\")\n",
    "\n",
    "# Basic Python modules\n",
    "import os, json, re, uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "print(\"‚úÖ Basic Python modules imported\")\n",
    "\n",
    "# Test dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"‚úÖ dotenv imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå dotenv failed: {e}\")\n",
    "\n",
    "# Test Azure\n",
    "try:\n",
    "    from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "    print(\"‚úÖ Azure identity imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Azure identity failed: {e}\")\n",
    "\n",
    "# Test LangChain\n",
    "try:\n",
    "    from langchain_openai import AzureChatOpenAI\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain.schema.output_parser import StrOutputParser\n",
    "    print(\"‚úÖ LangChain imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå LangChain failed: {e}\")\n",
    "\n",
    "# Test tiktoken\n",
    "try:\n",
    "    import tiktoken\n",
    "    print(\"‚úÖ tiktoken imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå tiktoken failed: {e}\")\n",
    "\n",
    "print(\"üéØ Import test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792703e",
   "metadata": {},
   "source": [
    "# üìö Progressive Book Summary Generator\n",
    "\n",
    "## üéØ **Objective**\n",
    "Create comprehensive book summaries using progressive chapter-by-chapter analysis with GPT-5-mini.\n",
    "\n",
    "## üîÑ **Progressive Summary Process**\n",
    "\n",
    "```\n",
    "Full Markdown Document\n",
    "         ‚Üì\n",
    "üìñ CHAPTER SPLITTING\n",
    "   Chapter 1, Chapter 2, Chapter 3...\n",
    "         ‚Üì\n",
    "üîÑ SEQUENTIAL SUMMARIZATION\n",
    "   Chapter 1 ‚Üí Summary 1\n",
    "   Chapter 2 + Summary 1 ‚Üí Summary 2  \n",
    "   Chapter 3 + Summary 1+2 ‚Üí Summary 3\n",
    "   ...\n",
    "         ‚Üì\n",
    "üìö FINAL BOOK SUMMARY\n",
    "   All Chapter Summaries ‚Üí Complete Book Overview\n",
    "```\n",
    "\n",
    "## üéì **Benefits**\n",
    "- **Progressive Context Building**: Each summary builds on previous knowledge\n",
    "- **Natural Reading Flow**: Mirrors human document comprehension\n",
    "- **Token Efficiency**: Manageable context windows at each step\n",
    "- **Quality Summaries**: Deep understanding through sequential analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae475a",
   "metadata": {},
   "source": [
    "## üìã Prerequisites\n",
    "\n",
    "**Before running this notebook:**\n",
    "1. Run `search_with_document_layout.ipynb` to generate markdown content\n",
    "2. Ensure GPT-5-mini deployment is configured\n",
    "3. Have markdown content ready for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5830782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "# Azure and LangChain imports\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import tiktoken\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e69fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded\n",
      "ü§ñ Using GPT-5-mini deployment: gpt-5-mini\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Azure OpenAI configs (GPT-5-mini)\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_CHAT_API_VERSION = os.getenv(\"AZURE_OPENAI_CHAT_API_VERSION\") or \"2024-12-01-preview\"\n",
    "\n",
    "print(f\"‚úÖ Environment loaded\")\n",
    "print(f\"ü§ñ Using GPT-5-mini deployment: {AZURE_OPENAI_CHAT_DEPLOYMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffb63b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3639: UserWarning: WARNING! max_completion_tokens is not default parameter.\n",
      "                max_completion_tokens was transferred to model_kwargs.\n",
      "                Please confirm that max_completion_tokens is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPT-5-mini client initialized\n",
      "üß† Context window: 272K input / 128K output\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure clients\n",
    "credential = DefaultAzureCredential()\n",
    "azure_ad_token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# Initialize GPT-5-mini client\n",
    "chat_llm = AzureChatOpenAI(\n",
    "    azure_deployment=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    openai_api_version=AZURE_OPENAI_CHAT_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token_provider=azure_ad_token_provider,\n",
    "    # temperature=1.0,  # GPT-5-mini only supports default temperature\n",
    "    max_completion_tokens=4000   # Allow for detailed summaries (GPT-5-mini parameter)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GPT-5-mini client initialized\")\n",
    "print(\"üß† Context window: 272K input / 128K output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a3d92",
   "metadata": {},
   "source": [
    "## üìñ Chapter Splitting and Processing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970b75ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data structures defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BookChapter:\n",
    "    \"\"\"Represents a chapter from the document\"\"\"\n",
    "    chapter_number: int\n",
    "    title: str\n",
    "    content: str\n",
    "    token_count: int\n",
    "    page_range: str\n",
    "    \n",
    "@dataclass\n",
    "class ChapterSummary:\n",
    "    \"\"\"Represents a generated chapter summary\"\"\"\n",
    "    chapter_number: int\n",
    "    chapter_title: str\n",
    "    summary: str\n",
    "    key_concepts: List[str]\n",
    "    main_topics: List[str]\n",
    "    token_count: int\n",
    "    created_at: datetime\n",
    "\n",
    "@dataclass \n",
    "class BookSummary:\n",
    "    \"\"\"Represents the final comprehensive book summary\"\"\"\n",
    "    book_title: str\n",
    "    overall_summary: str\n",
    "    chapter_summaries: List[ChapterSummary]\n",
    "    key_themes: List[str]\n",
    "    learning_objectives: List[str]\n",
    "    total_chapters: int\n",
    "    created_at: datetime\n",
    "\n",
    "print(\"‚úÖ Data structures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5a3cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ProgressiveBookSummarizer class defined\n"
     ]
    }
   ],
   "source": [
    "class ProgressiveBookSummarizer:\n",
    "    \"\"\"Progressive book summarization using sequential chapter analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.chapter_summaries = []\n",
    "        \n",
    "        # Progressive chapter summary prompt\n",
    "        self.chapter_summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert book summarizer creating progressive chapter summaries.\n",
    "\n",
    "CONTEXT FROM PREVIOUS CHAPTERS:\n",
    "{previous_context}\n",
    "\n",
    "CURRENT CHAPTER TO SUMMARIZE:\n",
    "Chapter: {chapter_number}\n",
    "Title: {chapter_title}\n",
    "Content:\n",
    "{chapter_content}\n",
    "\n",
    "Create a comprehensive summary that:\n",
    "1. **Builds on previous chapters** - Reference relevant concepts from earlier chapters\n",
    "2. **Identifies key concepts** - Extract main ideas and important terms\n",
    "3. **Explains relationships** - Show how this chapter connects to previous content\n",
    "4. **Highlights progression** - Show how understanding is building\n",
    "\n",
    "Format as JSON:\n",
    "{{\n",
    "    \"chapter_summary\": \"Detailed 2-3 paragraph summary building on previous chapters\",\n",
    "    \"key_concepts\": [\"concept1\", \"concept2\", \"concept3\"],\n",
    "    \"main_topics\": [\"topic1\", \"topic2\", \"topic3\"],\n",
    "    \"connections_to_previous\": \"How this chapter relates to previous chapters\",\n",
    "    \"new_insights\": \"What new understanding this chapter provides\"\n",
    "}}\n",
    "\"\"\")\n",
    "        \n",
    "        # Final book summary prompt\n",
    "        self.book_summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert book summarizer creating a comprehensive book overview.\n",
    "\n",
    "ALL CHAPTER SUMMARIES:\n",
    "{all_chapter_summaries}\n",
    "\n",
    "Create a comprehensive book summary that:\n",
    "1. **Overall narrative** - Tell the complete story of the book\n",
    "2. **Key themes** - Identify major themes running through the book\n",
    "3. **Learning progression** - Show how concepts build throughout\n",
    "4. **Practical value** - What readers will gain from this book\n",
    "\n",
    "Format as JSON:\n",
    "{{\n",
    "    \"book_title\": \"Inferred or provided book title\",\n",
    "    \"overall_summary\": \"Comprehensive 4-5 paragraph book summary\",\n",
    "    \"key_themes\": [\"theme1\", \"theme2\", \"theme3\"],\n",
    "    \"learning_objectives\": [\"objective1\", \"objective2\", \"objective3\"],\n",
    "    \"book_structure\": \"How the book is organized and flows\",\n",
    "    \"target_audience\": \"Who would benefit from this book\",\n",
    "    \"practical_applications\": \"How readers can apply this knowledge\"\n",
    "}}\n",
    "\"\"\")\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "    \n",
    "    def split_into_chapters(self, markdown_content: str) -> List[BookChapter]:\n",
    "        \"\"\"Split markdown content into logical chapters\"\"\"\n",
    "        print(\"üìñ Splitting document into chapters...\")\n",
    "        \n",
    "        # Split by markdown headers (# ## ###)\n",
    "        # This is a smart splitter that looks for natural chapter boundaries\n",
    "        chapter_pattern = r'^(#{1,3})\\s+(.+?)$'\n",
    "        sections = re.split(chapter_pattern, markdown_content, flags=re.MULTILINE)\n",
    "        \n",
    "        chapters = []\n",
    "        current_chapter = None\n",
    "        chapter_number = 0\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(sections):\n",
    "            if i + 2 < len(sections) and sections[i+1]:  # Found a header\n",
    "                header_level = sections[i+1]\n",
    "                title = sections[i+2].strip()\n",
    "                content = sections[i+3] if i+3 < len(sections) else \"\"\n",
    "                \n",
    "                # Only treat as new chapter if it's a major header (# or ##)\n",
    "                if len(header_level) <= 2:  # # or ##\n",
    "                    if current_chapter:  # Save previous chapter\n",
    "                        chapters.append(current_chapter)\n",
    "                    \n",
    "                    chapter_number += 1\n",
    "                    current_chapter = BookChapter(\n",
    "                        chapter_number=chapter_number,\n",
    "                        title=title,\n",
    "                        content=content.strip(),\n",
    "                        token_count=self.count_tokens(content),\n",
    "                        page_range=f\"Chapter {chapter_number}\"\n",
    "                    )\n",
    "                else:  # Subsection, add to current chapter\n",
    "                    if current_chapter:\n",
    "                        current_chapter.content += f\"\\n\\n{header_level} {title}\\n{content}\"\n",
    "                        current_chapter.token_count = self.count_tokens(current_chapter.content)\n",
    "                \n",
    "                i += 4\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # Add the last chapter\n",
    "        if current_chapter:\n",
    "            chapters.append(current_chapter)\n",
    "        \n",
    "        # If no clear chapters found, create artificial chapters by content length\n",
    "        if not chapters:\n",
    "            print(\"‚ö†Ô∏è  No clear chapter structure found, creating artificial chapters...\")\n",
    "            chapters = self._create_artificial_chapters(markdown_content)\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(chapters)} chapters\")\n",
    "        for i, chapter in enumerate(chapters, 1):\n",
    "            print(f\"   Chapter {i}: {chapter.title[:50]}... ({chapter.token_count:,} tokens)\")\n",
    "        \n",
    "        return chapters\n",
    "    \n",
    "    def _create_artificial_chapters(self, content: str, target_tokens: int = 15000) -> List[BookChapter]:\n",
    "        \"\"\"Create artificial chapters based on content length\"\"\"\n",
    "        words = content.split()\n",
    "        chapters = []\n",
    "        chapter_number = 0\n",
    "        \n",
    "        # Estimate words per chapter (roughly 4 tokens per word)\n",
    "        words_per_chapter = target_tokens // 4\n",
    "        \n",
    "        for i in range(0, len(words), words_per_chapter):\n",
    "            chapter_number += 1\n",
    "            chapter_words = words[i:i + words_per_chapter]\n",
    "            chapter_content = \" \".join(chapter_words)\n",
    "            \n",
    "            # Try to find a good title from the first few sentences\n",
    "            first_sentences = chapter_content[:200].split('. ')\n",
    "            title = f\"Section {chapter_number}: {first_sentences[0][:50]}...\"\n",
    "            \n",
    "            chapters.append(BookChapter(\n",
    "                chapter_number=chapter_number,\n",
    "                title=title,\n",
    "                content=chapter_content,\n",
    "                token_count=self.count_tokens(chapter_content),\n",
    "                page_range=f\"Section {chapter_number}\"\n",
    "            ))\n",
    "        \n",
    "        return chapters\n",
    "\n",
    "print(\"‚úÖ ProgressiveBookSummarizer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11fca0",
   "metadata": {},
   "source": [
    "## üîÑ Progressive Summarization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9da9398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Progressive summarization methods added\n"
     ]
    }
   ],
   "source": [
    "class ProgressiveBookSummarizer(ProgressiveBookSummarizer):\n",
    "    \"\"\"Extended class with progressive summarization methods\"\"\"\n",
    "    \n",
    "    def summarize_chapter_progressively(self, chapter: BookChapter, previous_summaries: List[ChapterSummary]) -> ChapterSummary:\n",
    "        \"\"\"Summarize a chapter with context from previous chapters\"\"\"\n",
    "        \n",
    "        print(f\"üìù Summarizing Chapter {chapter.chapter_number}: {chapter.title[:50]}...\")\n",
    "        \n",
    "        # Build context from previous summaries\n",
    "        previous_context = \"\"\n",
    "        if previous_summaries:\n",
    "            context_parts = []\n",
    "            for prev_summary in previous_summaries:\n",
    "                context_parts.append(f\"**Chapter {prev_summary.chapter_number}**: {prev_summary.summary}\")\n",
    "            previous_context = \"\\n\\n\".join(context_parts)\n",
    "        else:\n",
    "            previous_context = \"This is the first chapter - no previous context available.\"\n",
    "        \n",
    "        # Generate summary with progressive context\n",
    "        summary_chain = self.chapter_summary_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        response = summary_chain.invoke({\n",
    "            \"previous_context\": previous_context,\n",
    "            \"chapter_number\": chapter.chapter_number,\n",
    "            \"chapter_title\": chapter.title,\n",
    "            \"chapter_content\": chapter.content\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            summary_data = json.loads(response)\n",
    "            \n",
    "            chapter_summary = ChapterSummary(\n",
    "                chapter_number=chapter.chapter_number,\n",
    "                chapter_title=chapter.title,\n",
    "                summary=summary_data.get(\"chapter_summary\", \"\"),\n",
    "                key_concepts=summary_data.get(\"key_concepts\", []),\n",
    "                main_topics=summary_data.get(\"main_topics\", []),\n",
    "                token_count=self.count_tokens(summary_data.get(\"chapter_summary\", \"\")),\n",
    "                created_at=datetime.now()\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Chapter {chapter.chapter_number} summarized ({chapter_summary.token_count} tokens)\")\n",
    "            print(f\"   Key concepts: {', '.join(chapter_summary.key_concepts[:3])}...\")\n",
    "            \n",
    "            return chapter_summary\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è  JSON parsing failed for Chapter {chapter.chapter_number}, using raw response\")\n",
    "            return ChapterSummary(\n",
    "                chapter_number=chapter.chapter_number,\n",
    "                chapter_title=chapter.title,\n",
    "                summary=response,\n",
    "                key_concepts=[],\n",
    "                main_topics=[],\n",
    "                token_count=self.count_tokens(response),\n",
    "                created_at=datetime.now()\n",
    "            )\n",
    "    \n",
    "    def create_final_book_summary(self, chapter_summaries: List[ChapterSummary]) -> BookSummary:\n",
    "        \"\"\"Create comprehensive book summary from all chapter summaries\"\"\"\n",
    "        \n",
    "        print(\"üìö Creating final book summary from all chapters...\")\n",
    "        \n",
    "        # Compile all chapter summaries\n",
    "        all_summaries_text = \"\\n\\n\".join([\n",
    "            f\"**Chapter {cs.chapter_number}: {cs.chapter_title}**\\n{cs.summary}\\nKey Concepts: {', '.join(cs.key_concepts)}\"\n",
    "            for cs in chapter_summaries\n",
    "        ])\n",
    "        \n",
    "        # Generate final book summary\n",
    "        book_summary_chain = self.book_summary_prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        response = book_summary_chain.invoke({\n",
    "            \"all_chapter_summaries\": all_summaries_text\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            book_data = json.loads(response)\n",
    "            \n",
    "            book_summary = BookSummary(\n",
    "                book_title=book_data.get(\"book_title\", \"Document Summary\"),\n",
    "                overall_summary=book_data.get(\"overall_summary\", \"\"),\n",
    "                chapter_summaries=chapter_summaries,\n",
    "                key_themes=book_data.get(\"key_themes\", []),\n",
    "                learning_objectives=book_data.get(\"learning_objectives\", []),\n",
    "                total_chapters=len(chapter_summaries),\n",
    "                created_at=datetime.now()\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Book summary created: {book_summary.book_title}\")\n",
    "            print(f\"   Total chapters: {book_summary.total_chapters}\")\n",
    "            print(f\"   Key themes: {len(book_summary.key_themes)}\")\n",
    "            \n",
    "            return book_summary\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è  JSON parsing failed for book summary, using raw response\")\n",
    "            return BookSummary(\n",
    "                book_title=\"Document Summary\",\n",
    "                overall_summary=response,\n",
    "                chapter_summaries=chapter_summaries,\n",
    "                key_themes=[],\n",
    "                learning_objectives=[],\n",
    "                total_chapters=len(chapter_summaries),\n",
    "                created_at=datetime.now()\n",
    "            )\n",
    "    \n",
    "    def process_document_progressively(self, markdown_content: str) -> BookSummary:\n",
    "        \"\"\"Complete progressive book summarization process\"\"\"\n",
    "        \n",
    "        print(\"üöÄ Starting Progressive Book Summarization\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Split into chapters\n",
    "        chapters = self.split_into_chapters(markdown_content)\n",
    "        \n",
    "        # Step 2: Progressive chapter summarization\n",
    "        print(\"\\nüìù Progressive Chapter Summarization\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        chapter_summaries = []\n",
    "        \n",
    "        for chapter in chapters:\n",
    "            # Summarize with context of all previous summaries\n",
    "            chapter_summary = self.summarize_chapter_progressively(chapter, chapter_summaries)\n",
    "            chapter_summaries.append(chapter_summary)\n",
    "            \n",
    "            print(f\"   Context now includes {len(chapter_summaries)} chapters\")\n",
    "        \n",
    "        # Step 3: Create final book summary\n",
    "        print(\"\\nüìö Final Book Summary Generation\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        book_summary = self.create_final_book_summary(chapter_summaries)\n",
    "        \n",
    "        print(\"\\nüéØ Progressive Summarization Complete!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return book_summary\n",
    "\n",
    "print(\"‚úÖ Progressive summarization methods added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f825db",
   "metadata": {},
   "source": [
    "## üß™ Testing and Demo Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aef71fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Progressive Book Summarizer...\n",
      "‚úÖ Progressive Book Summarizer ready!\n",
      "\n",
      "üí° Usage:\n",
      "   # Load your markdown content from Content Understanding\n",
      "   # markdown_content = load_your_markdown_here()\n",
      "   # book_summary = summarizer.process_document_progressively(markdown_content)\n",
      "\n",
      "üéØ This will create progressive chapter summaries and a final book summary\n",
      "‚úÖ Progressive Book Summarizer ready!\n",
      "\n",
      "üí° Usage:\n",
      "   # Load your markdown content from Content Understanding\n",
      "   # markdown_content = load_your_markdown_here()\n",
      "   # book_summary = summarizer.process_document_progressively(markdown_content)\n",
      "\n",
      "üéØ This will create progressive chapter summaries and a final book summary\n"
     ]
    }
   ],
   "source": [
    "# Initialize the progressive book summarizer\n",
    "print(\"üöÄ Initializing Progressive Book Summarizer...\")\n",
    "\n",
    "summarizer = ProgressiveBookSummarizer(chat_llm)\n",
    "\n",
    "print(\"‚úÖ Progressive Book Summarizer ready!\")\n",
    "print(\"\\nüí° Usage:\")\n",
    "print(\"   # Load your markdown content from Content Understanding\")\n",
    "print(\"   # markdown_content = load_your_markdown_here()\")\n",
    "print(\"   # book_summary = summarizer.process_document_progressively(markdown_content)\")\n",
    "print(\"\\nüéØ This will create progressive chapter summaries and a final book summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee2ea0",
   "metadata": {},
   "source": [
    "## üíæ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bad6818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Save function ready!\n",
      "üí° Usage: save_book_summary(your_book_summary, custom_filename='custom_name.json')\n"
     ]
    }
   ],
   "source": [
    "# Save book summary to file\n",
    "def save_book_summary(book_summary: BookSummary, output_path: str = \"../educational_content/book_summaries\", custom_filename: str = None):\n",
    "    \"\"\"Save book summary to JSON file\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(output_path)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create filename with timestamp or use custom filename\n",
    "    if custom_filename:\n",
    "        filename = custom_filename\n",
    "    else:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"book_summary_{timestamp}.json\"\n",
    "    \n",
    "    filepath = output_dir / filename\n",
    "    \n",
    "    # Convert to dictionary for JSON serialization\n",
    "    summary_dict = {\n",
    "        \"book_title\": book_summary.book_title,\n",
    "        \"overall_summary\": book_summary.overall_summary,\n",
    "        \"key_themes\": book_summary.key_themes,\n",
    "        \"learning_objectives\": book_summary.learning_objectives,\n",
    "        \"total_chapters\": book_summary.total_chapters,\n",
    "        \"created_at\": book_summary.created_at.isoformat(),\n",
    "        \"chapter_summaries\": [\n",
    "            {\n",
    "                \"chapter_number\": cs.chapter_number,\n",
    "                \"chapter_title\": cs.chapter_title,\n",
    "                \"summary\": cs.summary,\n",
    "                \"key_concepts\": cs.key_concepts,\n",
    "                \"main_topics\": cs.main_topics,\n",
    "                \"token_count\": cs.token_count,\n",
    "                \"created_at\": cs.created_at.isoformat()\n",
    "            } for cs in book_summary.chapter_summaries\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_dict, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üíæ Book summary saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "print(\"‚úÖ Save function ready!\")\n",
    "print(\"üí° Usage: save_book_summary(your_book_summary, custom_filename='custom_name.json')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3513ff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö PROGRESSIVE BOOK SUMMARIZATION - REAL DOCUMENT TEST\n",
      "================================================================================\n",
      "üìÑ Loading: ../educational_content/venture_deals_markdown.md\n",
      "‚ùå File not found: ../educational_content/venture_deals_markdown.md\n",
      "Please make sure the Content Understanding pipeline has been run first.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ REAL DOCUMENT TEST: Process Venture Deals with Progressive Summarization\n",
    "print(\"üìö PROGRESSIVE BOOK SUMMARIZATION - REAL DOCUMENT TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load the real Venture Deals markdown content\n",
    "venture_deals_path = \"../educational_content/venture_deals_markdown.md\"\n",
    "print(f\"üìÑ Loading: {venture_deals_path}\")\n",
    "\n",
    "try:\n",
    "    with open(venture_deals_path, 'r', encoding='utf-8') as f:\n",
    "        venture_deals_content = f.read()\n",
    "    \n",
    "    print(f\"‚úÖ Content loaded successfully!\")\n",
    "    print(f\"üìä Content statistics:\")\n",
    "    print(f\"   üìù Characters: {len(venture_deals_content):,}\")\n",
    "    print(f\"   üìÑ Lines: {venture_deals_content.count(chr(10)) + 1:,}\")\n",
    "    print(f\"   üßÆ Estimated tokens: {len(venture_deals_content.split()) * 1.3:.0f}\")\n",
    "    \n",
    "    # Show first few lines as preview\n",
    "    first_lines = venture_deals_content.split('\\n')[:10]\n",
    "    print(f\"\\\\nüìñ Content preview (first 10 lines):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, line in enumerate(first_lines, 1):\n",
    "        print(f\"{i:2d}: {line[:80]}...\")\n",
    "    \n",
    "    print(\"\\\\nüéØ Ready to run Progressive Book Summarization!\")\n",
    "    print(\"‚ö†Ô∏è  Warning: This is a large document (~135K tokens) - processing may take 10-15 minutes\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {venture_deals_path}\")\n",
    "    print(\"Please make sure the Content Understanding pipeline has been run first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e69d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• STARTING PROGRESSIVE SUMMARIZATION - VENTURE DEALS\n",
      "================================================================================\n",
      "üìö Processing: 'Venture Deals' by Brad Feld & Jason Mendelson\n",
      "‚è±Ô∏è  Estimated time: 10-15 minutes for ~135K tokens\n",
      "üß† Using GPT-5-mini with 272K context window\n",
      "\n",
      "üöÄ Beginning progressive summarization...\n",
      "‚ùå Error during processing: name 'venture_deals_content' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_27596/3581662100.py\", line 16, in <module>\n",
      "    venture_deals_summary = summarizer.process_document_progressively(venture_deals_content)\n",
      "                                                                      ^^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'venture_deals_content' is not defined\n"
     ]
    }
   ],
   "source": [
    "# üöÄ RUN Progressive Summarization on Venture Deals\n",
    "print(\"üî• STARTING PROGRESSIVE SUMMARIZATION - VENTURE DEALS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üìö Processing: 'Venture Deals' by Brad Feld & Jason Mendelson\")\n",
    "print(\"‚è±Ô∏è  Estimated time: 10-15 minutes for ~135K tokens\")\n",
    "print(\"üß† Using GPT-5-mini with 272K context window\")\n",
    "print(\"\")\n",
    "\n",
    "# Start timing\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Run the progressive book summarization\n",
    "    print(\"üöÄ Beginning progressive summarization...\")\n",
    "    venture_deals_summary = summarizer.process_document_progressively(venture_deals_content)\n",
    "    \n",
    "    # Calculate processing time\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    print(\"\\\\nüéâ PROGRESSIVE SUMMARIZATION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"‚è±Ô∏è  Total processing time: {processing_time/60:.1f} minutes\")\n",
    "    print(f\"üìö Book title: {venture_deals_summary.book_title}\")\n",
    "    print(f\"üìñ Total chapters: {venture_deals_summary.total_chapters}\")\n",
    "    print(f\"üéØ Key themes: {len(venture_deals_summary.key_themes)}\")\n",
    "    print(f\"üéì Learning objectives: {len(venture_deals_summary.learning_objectives)}\")\n",
    "    \n",
    "    # Show summary preview\n",
    "    print(f\"\\\\nüìÑ Book Summary Preview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(venture_deals_summary.overall_summary[:500] + \"...\")\n",
    "    \n",
    "    print(f\"\\\\nüîë Key Themes:\")\n",
    "    for i, theme in enumerate(venture_deals_summary.key_themes[:5], 1):\n",
    "        print(f\"   {i}. {theme}\")\n",
    "    if len(venture_deals_summary.key_themes) > 5:\n",
    "        print(f\"   ... and {len(venture_deals_summary.key_themes) - 5} more\")\n",
    "    \n",
    "    print(\"\\\\n‚úÖ Ready to save comprehensive book summary!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c10abe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVING VENTURE DEALS BOOK SUMMARY\n",
      "============================================================\n",
      "‚ùå venture_deals_summary not found - processing may have failed\n"
     ]
    }
   ],
   "source": [
    "# üíæ Save the Venture Deals Book Summary\n",
    "print(\"üíæ SAVING VENTURE DEALS BOOK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'venture_deals_summary' in locals():\n",
    "    # Save the comprehensive book summary with custom filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"venture_deals_book_summary_{timestamp}.json\"\n",
    "    filepath = save_book_summary(venture_deals_summary, \"../educational_content/book_summaries\", filename)\n",
    "    \n",
    "    print(f\"\\nüìö VENTURE DEALS SUMMARY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìñ Book Title: {venture_deals_summary.book_title}\")\n",
    "    print(f\"üìÑ Total Chapters: {venture_deals_summary.total_chapters}\")\n",
    "    print(f\"üéØ Key Themes: {len(venture_deals_summary.key_themes)}\")\n",
    "    print(f\"üéì Learning Objectives: {len(venture_deals_summary.learning_objectives)}\")\n",
    "    \n",
    "    print(f\"\\nüîë Key Themes Identified:\")\n",
    "    for i, theme in enumerate(venture_deals_summary.key_themes, 1):\n",
    "        print(f\"   {i:2d}. {theme}\")\n",
    "    \n",
    "    print(f\"\\nüéì Learning Objectives:\")\n",
    "    for i, objective in enumerate(venture_deals_summary.learning_objectives, 1):\n",
    "        print(f\"   {i:2d}. {objective}\")\n",
    "    \n",
    "    print(f\"\\nüìã Chapter Breakdown:\")\n",
    "    for i, chapter_summary in enumerate(venture_deals_summary.chapter_summaries[:5], 1):\n",
    "        print(f\"   Chapter {chapter_summary.chapter_number}: {chapter_summary.chapter_title[:60]}...\")\n",
    "        print(f\"      Key concepts: {', '.join(chapter_summary.key_concepts[:3])}...\")\n",
    "    \n",
    "    if len(venture_deals_summary.chapter_summaries) > 5:\n",
    "        print(f\"   ... and {len(venture_deals_summary.chapter_summaries) - 5} more chapters\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Book Summary (first 300 characters):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(venture_deals_summary.overall_summary[:300] + \"...\")\n",
    "    \n",
    "    print(f\"\\nüéâ SUCCESS! Complete progressive book summary generated and saved!\")\n",
    "    print(f\"üíæ Saved to: {filepath}\")\n",
    "else:\n",
    "    print(\"‚ùå venture_deals_summary not found - processing may have failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5941cd1",
   "metadata": {},
   "source": [
    "## üöÄ How to Use the New Production Summarizer\n",
    "\n",
    "Now that we have a production-ready implementation, you have several options to summarize markdown content:\n",
    "\n",
    "### üéØ Option 1: Use the Production Implementation Directly\n",
    "\n",
    "The recommended approach is to use our new production code that works exactly like this notebook:\n",
    "\n",
    "```python\n",
    "# Import the production summarizer\n",
    "from src.summarization.book_summarizer import ProgressiveBookSummarizer\n",
    "\n",
    "# Initialize (automatically handles Azure OpenAI setup)\n",
    "summarizer = ProgressiveBookSummarizer()\n",
    "\n",
    "# Process your markdown content\n",
    "result = summarizer.process_document_progressively(\n",
    "    markdown_content=your_markdown_content,\n",
    "    book_title=\"Your Book Title\"\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"Book Title: {result.book_title}\")\n",
    "print(f\"Summary: {result.overall_summary}\")\n",
    "print(f\"Key Themes: {result.key_themes}\")\n",
    "print(f\"Learning Objectives: {result.learning_objectives}\")\n",
    "```\n",
    "\n",
    "### üåê Option 2: Use the REST API\n",
    "\n",
    "Start the API server and use HTTP endpoints:\n",
    "\n",
    "```bash\n",
    "# Start the server\n",
    "python run_server.py\n",
    "\n",
    "# Option 2a: Send content directly\n",
    "curl -X POST http://localhost:8000/summarization/summarize \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"book_title\": \"My Book\", \n",
    "    \"markdown_content\": \"# Chapter 1\\nYour content here...\"\n",
    "  }'\n",
    "\n",
    "# Option 2b: Upload a markdown file\n",
    "curl -X POST http://localhost:8000/summarization/summarize-file \\\n",
    "  -F \"file=@your_book.md\" \\\n",
    "  -F \"book_title=My Book\"\n",
    "```\n",
    "\n",
    "### üß™ Option 3: Continue Using This Notebook\n",
    "\n",
    "You can still use this notebook implementation - both work identically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c39a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ PRACTICAL EXAMPLE: Using the New Production Summarizer\n",
    "print(\"üöÄ USING THE NEW PRODUCTION SUMMARIZER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Import and use the production implementation directly\n",
    "try:\n",
    "    # Add the src directory to Python path\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "    \n",
    "    # Import the production summarizer\n",
    "    from src.summarization.book_summarizer import ProgressiveBookSummarizer\n",
    "    \n",
    "    print(\"‚úÖ Production summarizer imported successfully!\")\n",
    "    \n",
    "    # Create a test markdown example\n",
    "    test_markdown = \"\"\"# Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task.\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "At its core, machine learning involves training algorithms on data to recognize patterns and make predictions or decisions on new, unseen data.\n",
    "\n",
    "### Types of Machine Learning\n",
    "\n",
    "1. **Supervised Learning**: Learning with labeled examples\n",
    "2. **Unsupervised Learning**: Finding patterns in unlabeled data  \n",
    "3. **Reinforcement Learning**: Learning through trial and error\n",
    "\n",
    "## Applications\n",
    "\n",
    "Machine learning powers many technologies we use daily:\n",
    "- Recommendation systems (Netflix, Amazon)\n",
    "- Image recognition (photo tagging)\n",
    "- Natural language processing (chatbots)\n",
    "- Autonomous vehicles\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "To begin your machine learning journey, you'll need to understand:\n",
    "- Basic statistics and probability\n",
    "- Programming (Python/R)\n",
    "- Data manipulation and visualization\n",
    "- Algorithm fundamentals\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Machine learning is transforming industries and creating new possibilities for solving complex problems through data-driven insights.\n",
    "\"\"\"\n",
    "\n",
    "    # Initialize the production summarizer (it handles Azure OpenAI setup automatically)\n",
    "    prod_summarizer = ProgressiveBookSummarizer()\n",
    "    \n",
    "    print(\"üß™ Testing with sample markdown content...\")\n",
    "    print(f\"üìÑ Content length: {len(test_markdown)} characters\")\n",
    "    \n",
    "    # Process the markdown content\n",
    "    result = prod_summarizer.process_document_progressively(\n",
    "        markdown_content=test_markdown,\n",
    "        book_title=\"Introduction to Machine Learning\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ PRODUCTION SUMMARIZER RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìñ Book Title: {result.book_title}\")\n",
    "    print(f\"üìö Total Chapters: {result.total_chapters}\")\n",
    "    print(f\"üéØ Key Themes ({len(result.key_themes)}):\")\n",
    "    for i, theme in enumerate(result.key_themes, 1):\n",
    "        print(f\"   {i}. {theme}\")\n",
    "    \n",
    "    print(f\"\\nüìù Summary (first 200 chars):\")\n",
    "    print(result.overall_summary[:200] + \"...\")\n",
    "    \n",
    "    print(\"\\nüéâ SUCCESS! The production summarizer works perfectly!\")\n",
    "    print(\"üí° You can now use this production code anywhere in your project!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Production summarizer not available: {e}\")\n",
    "    print(\"üí° You can still use the notebook implementation above!\")\n",
    "    print(\"üîß To fix this, make sure you've run the migration setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f43bf",
   "metadata": {},
   "source": [
    "## üìö Quick Reference: Summarizing Your Markdown\n",
    "\n",
    "### For Your Specific Use Case:\n",
    "\n",
    "1. **If you have a markdown file ready:**\n",
    "   ```python\n",
    "   # Load your markdown content\n",
    "   with open(\"path/to/your/book.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "       markdown_content = f.read()\n",
    "   \n",
    "   # Summarize it\n",
    "   from src.summarization.book_summarizer import ProgressiveBookSummarizer\n",
    "   summarizer = ProgressiveBookSummarizer()\n",
    "   result = summarizer.process_document_progressively(markdown_content, \"Your Book Title\")\n",
    "   ```\n",
    "\n",
    "2. **If you want to use the API:**\n",
    "   ```bash\n",
    "   # Start server in terminal: python run_server.py\n",
    "   # Then upload your file:\n",
    "   curl -X POST http://localhost:8000/summarization/summarize-file \\\n",
    "     -F \"file=@your_book.md\" -F \"book_title=Your Book\"\n",
    "   ```\n",
    "\n",
    "3. **If you want to continue with this notebook:**\n",
    "   - Just run the cells above to use the notebook implementation\n",
    "   - Both implementations work identically!\n",
    "\n",
    "### üéØ The production summarizer gives you:\n",
    "- ‚úÖ Progressive chapter-by-chapter analysis\n",
    "- ‚úÖ Comprehensive book summaries with themes and objectives  \n",
    "- ‚úÖ JSON-structured responses\n",
    "- ‚úÖ Ready for deployment and scaling\n",
    "- ‚úÖ Works exactly like this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
